#+OPTIONS: ^:{} _:{} num:t toc:t \n:t
#+include "../../template.org"
#+title:wget下载


* OPTIONS
** -r
   --recursive
   对于HTTP主机，wget首先下载URL指定的文件，然后（如果该文件是一个HTML文档的话）递归下载该文件所引用（超级连接）的所有文件（递归深度由参数-l指定）
   对FTP主机，该参数意味着要下载URL指定的目录中的所有文件，递归方法与HTTP主机类似。
** -l depth
   --level=depth
   指定递归的最大深度
** -k
   --covert-links
   转化文档中的链接为本地链接
** -O file
   --output-document=file
   保存文件名
** -o logfile
   --output-file=logfile
   所有输出信息记录到logfile中
** --limit-rate=amount
   后跟限制下载速度
** -c
   --continue
   续传（下载一个大文件时突然发生中断，可以继续下载）
** -b
   --background
   后台下载，产生wget-log文件显示日志
** --spider
   后跟下载链接用来测试链接是否可用
** -t number
   --tries=number
   指定重试次数
** -T seconds
   --timeout=seconds
   超过seconds则停止
** -q
   --quiet
   关闭输出
** -v
   --verbose
   详细输出，默认
** -i file
   --input-file=file
   下载链接在file文件中
** -B URL
   -base=URL
   基址
** -A acclist
   --accept acclist
** -R rejlist
   --reject reglist
   使用逗号分隔的文件后缀或者通配符列表
** --user=user
** --password=password
   用户名和密码，可以用于ftp和http，--ftp-user=user --ftp-password=password --http-user=user --http-passwor
** -p prefix
   --directory-prefix=prefix
   将下载文件保存在指定prefix中，默认是.即当前目录
** -m
   --mirror
   下载事件网站 wget -m fanhan.tk
* 例子
** 下载网页
   wget fanhan.tk
   原封不动地下载该网站的index.html，但打开后发现由于路径问题链接的css路径 css/vf.css 不存在所以页面根本没法看，于是我们可以用-k选项将链接转换成适合在本地查看的形式，
   wget -k fanhan.tk
   此时可以看到 css/vf.css转变成了http://fanhan.tk/css/vf.css 于是打开index.html时浏览器会下载该文件以显示页面
** 下载网站
   wget -m fanhan.tk
   把整个网站都下载下来
** 下载文件并重命名
   wget -O emacs.tar.gz http://ftp.gnu.org/gnu/emacs/emacs-24.1.tar.gz
** 下载所有匹配文件
   wget -r -A “*.tar.gz" http://ftp.gnu.org/gnu/emacs
   下载http://ftp.gnu.org/gnu/emacs 目录下的所有以tar.gz结尾的文件
#+BEGIN_HTML
<script src="../../Layout/JS/disqus-comment.js"></script>
<div id="disqus_thread">
</div>
#+END_HTML
